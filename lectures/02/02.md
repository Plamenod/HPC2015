
##Single core optimizations

<img src="../misc_images/deadpool.png" width="200px">

---

##Recap of what we talked in the last lectures ...

---

#Single core optimizations

---

Types of Data Hazards

Data-dependence (**Read-after-Write**):
0. a + b -> c
1. c + d -> e

Anti-dependence (**Write-after-Read**):
0. a + b -> c
1. d + e -> a

Output-dependence (**Write-after-Write**):
0. a + b -> c
1. d + e -> c

What about (Read-after-Read) ?

---

#[DEMO ILP]

#[DEMO CACHES]

---

Translation Look-aside Buffer (**TLB**) maintains a small list of frequently used memory pages and their locations; addressing data that are location on one of these pages is much faster than data that are not. Consequently, one wants to code in such a way that the number of pages accessed is kept low.

What is the **page size** in the modern OSs ?

*if we have time today, we will check how custom allocators can affect the TLB cache performance*

---

Branch Predictor
```
if (a) {
    //...
} else {
    //...
}
```

```
eval(A)

if (!A) {
    jmp p1
}

//code A
jmp p2
p1:
... code !A
p2:
```

---

```
#if !defined(_MSC_VER) || defined(__INTEL_COMPILER)
    #define   likely(expr) __builtin_expect((expr),true )
    #define unlikely(expr) __builtin_expect((expr),false)
#endif //!defined(_MSC_VER) || defined(__INTEL_COMPILER)
```

```
while (likely(flag1 == 0)) { ... }
//
if (unlikely(x == 0)) { ... }
```

---

Pointer aliasing

![pointer aliasing](../01/images/pointer_aliasing.png)

---

```
void add(int* restrict ptrA, 
         int* restrict ptrB, 
         int* restrict val) 
{
    *ptrA += *val;
    *ptrB += *val;
}
```

C++ ? Fortran ?

Can function inlining affect that ? 

---

#[DEMO POINTER ALIAS]

---

IF vs SWITCH
```
switch(i) {
    case 0: doZero(); break
    case 1: doOne();
    case 2: doTwo(); break
    default: doOther();
}
```

```
if greater, jmp to DEFAULT
compare REG to 0
if less jmp to DEFAULT
jmp to table[REG]
data table
ZERO
ONE
TWO
end data
ZERO: call doZero
jmp END
ONE: call doOne
TWO: call doTwo
jmp END
DEFAULT: call doDefault
END:
```

---

Sometimes compiler makes jump table from IFs too.

As usual, if this is really important, don't trust the compiler and code the jump table by yourself.

---

What about i++ vs ++i ?

```
self_type operator++() { 
    self_type i = *this; 
    ptr_++;
    return i;
}

self_type operator++(int junk) { 
    ptr_++; 
    return *this; 
}
```

Usually doesn't matter, compiler are often smart.

But go for **++i**.

---

Inlining is the root of all optimizations

```
void changeSaturation(T *R, T *G, T *B, T change) {
    T P=sqrt(
        (*R)*(*R)*Pr+
        (*G)*(*G)*Pg+
        (*B)*(*B)*Pb ) ;

    *R=P+((*R)-P)*change;
    *G=P+((*G)-P)*change;
    *B=P+((*B)-P)*change; 
}
```

```
float r, g, b, c;
r = g = b = c = 0.f;
changeSaturation(&r, &g, &b, &c);
printf("%f\n", r + g + b + c);
```

---

#[DEMO INLINING]

---

So why just don't we inline everything ?

---

All of this - caches, branch predictor, out of order execution, register renaming, etc is done to hide the latency to the memory.

Are there any other completely different approaches ?

---

#COMPILERS

---

Compilers 

* cl (MSVS)

* icc (Intel)

* gcc 

* clang 

* others

---


<img style="float: left;" src="../01/images/compiler1.png" alt="compiler dragoon book" width="380px">

<img style="float: right;" src="../01/images/compiler2.png" width="380px">

---

<img style="float: right;" src="../01/images/compiler3.png" width="380px">

* Link time code generation 

* Whole program optimization 

* Link Time Optimization


---


All in one cpp

```
//file unity_build.cpp
#include "first.cpp"
#include "second.cpp"
//...
#include "last.cpp"
```

Pros : 0 link time, fast compilation, possibly faster code (why?)

Cons : hard to maintain

---

![](./images/ir.png)

---

Memory alignment
```
for (int i = 0; i < 4096; ++i) {
    printf("%p\n", new int);
}
```

```
template <typename T>
struct Node {
    T* left; T* right;
    bool something, somethingElse;
};

template<typename T>
struct Node {
T* left, right;
Т* getLeft() const { //make getRight() too
    T* tempPtr = left;
    setFlag(tempPtr, 0, 0);
    setFlag(tempPtr, 1, 0);
    return tempPtr;
}
bool getSomething() const { return getFlag(left, 0); }
bool getSomethingElse() const { return getFlag(left, 1); }
void setSomething(bool foo) { setFlag(left, 0, foo); }
void setSomethingElse(bool foo) { setFlag(left, 1, foo); }
};
```

---

We have to reset the last 2 bits every time when we use the ptr.

Some architectures are **ignoring** them.

---

To const or not to const ?

```
float temp = a + b;
for (int i = 0; i < size; ++i)
    fooBar(temp)
```

```
const float temp = a + b;
for (int i = 0; i < size; ++i)
    fooBar(temp)
```

---

It doesn't matter for the compiler

(the compiler knows better than you what is const and what is not)

**const** could however make the code safer, so use it

---

```
const float input[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 };
float output[COUNT_OF(input)];
int main(int, char**) {
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

```
float input[] = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 };
float output[COUNT_OF(input)];
int main(int, char**) {
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

---

```
typedef const float (& immutable_array_of_16_floats)[16];
struct InputArray {
    float array[2];
    InputArray() { array[0] = 1; array[1] = 2; }
    operator immutable_array_of_16_floats () const {
        return array;
    }
};
const InputArray input;
float output[2];
int main(int, char**){
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

---

```
#include <iostream> //malloc(196);
typedef const float (& immutable_array_of_16_floats)[16];
struct InputArray {
    float array[2];
    InputArray() { array[0] = 1; array[1] = 2; }
    operator immutable_array_of_16_floats () const {
        return array;
    }
};
const InputArray input;
float output[2];
int main(int, char**){
    for (size_t i = 0; i < COUNT_OF(input); ++i)
        output[i] = input[i] + input[i];
}
```

---

#DEMO [CONST PROP]

---

LLVM coding style guide


```
#include <iostream>

std::cout << "Hello world";
```


>The use of #include <iostream> in library files is **hereby forbidden**

Why ?

---


```
float x, y, z;
x = atan2(e0, e1);
y = atan2(e0, e1);
z = atan2(e0, e1);
```

Is there a problem with that ?

```
for (int i = 0; i < size; ++i) {
    //..
    x += sinf(y);
    //...
}
```

---


What about **virtual** ? How is it implemented ?

Usually compiler transfer **foo.bar()** to [jump] and **foo.x** to [addr + offsetof(x)]. Where should virtual jump is known runtime only.

![virtual memory layout](../01/images/layout.png)

---

1st 64bit used for pointer to vtable (= cache miss)

can't be inlined (since what has to be executed is know only runtime)

```
#define SLOW virtual
class Bar {
    SLOW void test() const;
};
```

The **virtual** keyword is the fundament of OOP.

Think **twice**     before using OOP (more on that after few weeks)

---

#[DEMO VIRTUAL]

---

Devirtualizer

```
struct A {
    virtual int foo (void) {return 42;}
};
int test() {
    struct A a, *b=&a;
    return b->foo();
}
```

Inlining + Constant propagation + Dead code removal + Inliner + Dead code removal

```
int test() () {
    return 42;
}
```

---

How can we improve the devirtualization ?

---

#STD SORT CASE STUDY

---

```
__restart:
difference_type __len = __last - __first;
switch (__len) {
    case 0: case 1:
        return;
    case 2:
        if (__comp(*--__last, *__first))
            swap(*__first, *__last);
        return;
    case 3:
        _VSTD::__sort3<_Compare>(__first, __first+1, --__last, __comp);
        return;
    case 4:
        _VSTD::__sort4<_Compare>(__first, __first+1, __first+2, --__last, __comp);
        return;
    case 5:
        _VSTD::__sort5<_Compare>(__first, __first+1, __first+2, __first+3, --__last, __comp);
        return;
    }
// const difference_type __limit = is_trivially_copy_constructible<value_type>::value && is_trivially_copy_assignable<value_type>::value ? 30 : 6;
    if (__len <= __limit) {
        _VSTD::__insertion_sort_3<_Compare>(__first, __last, __comp);
        return ;
    }
    //etcß
```

---

32bit vs 64bit CPUs

![](./images/tegra64.png)

---

Conclusion

* Single threaded performance is not getting any faster

* In the past it was hard to make complex app, but it was getting faster year after year.

* Now, sophisticated apps are easier to make (use SophisticatedAdd.js). Fast apps are *not*.

* Most of the people think that the computers are so fast, that there is no point in in-depth optimizations. As we saw, it is exactly *the opposite*.

* You need max performance per thread, use all the threads & use any extra compute units if you want to do more.

---

# Q&A

---

Note:
Since I am not sure how much time the above will take, some bonus slides for OS

#OS

---

What is the OS doing ?

How can we have multiple processes when we have one core ?

How can we have multiple threads when we have one core ?

What is a **system process** and what is a **user process** ?

How does this affect the performance of your programs ?

---

How are drivers implemented ?

What happens when we call **fopen(...)** ?

How does this affect the performance of your programs ?

---

How does the scheduling of the processes happens ?

How often is it being done ?

How does this affect the performance of your programs ?

---

What is megakernel OS and what is microkernel OS ?

What kind of OSes are OS X, Windows, Linux, iOS, Android ?

How does this affect the performance of your programs ?

---

What about consoles ?
